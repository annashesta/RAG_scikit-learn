{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74166638-9fcd-4671-bb7b-c9ac90ffbc32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c845aae-4403-409e-b48e-3f4aeeb3ab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка зависимостей (если нужно)\n",
    "# Установка необходимых пакетов\n",
    "# !pip install langchain langchain-community langchain-core langchain-ollama \n",
    "# !pip installpypdf faiss-cpu sentence-transformers ollama beautifulsoup4 \n",
    "# !pip installunstructured lxml nltk tqdm dill gradio requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7ee0424-d6a9-4d75-b87e-595ca4f02a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  На мак faiss-cpu через анаконду :\n",
    "# conda install -c conda-forge faiss-cpu\n",
    "# import faiss\n",
    "# print(faiss.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce4a103-9b31-4815-9d17-8cff702444da",
   "metadata": {},
   "source": [
    "Общий план:\n",
    "1. Эмбеддинги — используем bge-m3 (только для векторизации!)\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(model=\"bge-m3\")\n",
    "\n",
    "2. LLM для генерации — используем модель, которая умеет отвечать\n",
    "from langchain_ollama import ChatOllama\n",
    "llm = ChatOllama(model=\"llama3\", temperature=0.3)  # или mistral, phi3 и т.д.\n",
    "\n",
    "3. Полученные чанки и вектора сохранены, можно загрузить сохранённые чанки и векторное хранилище, чтобы не перезапускать всю предобработку и векторизацию заново, достаточно раскомментировать и выполнить ячейку для перезапуска.\n",
    "\n",
    "Материалы:\n",
    "https://python.langchain.com/docs/tutorials/rag/\n",
    "\n",
    "Курс по RAG\n",
    "- https://www.youtube.com/playlist?list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x\n",
    "- https://github.com/langchain-ai/rag-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7462682c-e68d-42f6-8366-571ed4d18fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b7dfa19-cde3-42b9-9016-b061a28da944",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODOO Убедиться, что ollama запущен: В терминале: `ollama run llama3` или `ollama run bge-m3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "6b165fdb-da4a-487a-8252-dfc640e32246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ssl\n",
    "import sys \n",
    "import time\n",
    "import dill\n",
    "import pickle\n",
    "import requests\n",
    "import zipfile\n",
    "from typing import List\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# LangChain Core\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# LangChain Text Splitting\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# LangChain Ollama Integration\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "\n",
    "# LangChain Vector Store\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Gradio (UI)\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ca3106-dbdd-413d-be21-40969f51074a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ячейка для перезапуска\n",
    "# # Убедитесь, что ollama запущена — без нее не загрузится OllamaEmbeddings\n",
    "# # В терминале: ollama run bge-m3\n",
    "\n",
    "# # Загрузка чанков\n",
    "# if os.path.exists(\"output/scikit_chunks.pkl\"):\n",
    "#     with open(\"output/scikit_chunks.pkl\", \"rb\") as f:\n",
    "#         processed_chunks = dill.load(f)\n",
    "#     print(f\"✅ Загружено {len(processed_chunks)} чанков\")\n",
    "# else:\n",
    "#     print(\"Чанки не найдены. Запустите предобработку.\")\n",
    "\n",
    "# # Загрузка векторного хранилища\n",
    "# embeddings = OllamaEmbeddings(model=\"bge-m3\")\n",
    "# index_path = \"output/scikit_faiss_index\"\n",
    "\n",
    "# if os.path.exists(index_path):\n",
    "#     db = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "#     retriever = db.as_retriever(search_kwargs={\"k\": 4})\n",
    "#     print(\"✅ Векторное хранилище загружено\")\n",
    "# else:\n",
    "#     print(f\"Индекс не найден: {index_path}\")\n",
    "\n",
    "# # Проверка\n",
    "# if 'db' in locals():\n",
    "#     docs = db.similarity_search(\"clustering\", k=1)\n",
    "#     print(f\"\\nПример извлечённого чанка:\\n{docs[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b72124-6d40-408b-a765-ca8fa20ee675",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a4d2c7c-18fd-42d4-83da-ff48a7213c05",
   "metadata": {},
   "source": [
    "# 1. Загрузка (Load)\n",
    "Документация Scikit-learn https://scikit-learn.org/dev/versions.html \n",
    "- Актуальная версия называется stable-документация\n",
    "- Наданный момент актуальна Scikit-learn 1.7.1\n",
    "\n",
    "К сожалению, сайт scikit-learn.org блокирует попытки парсинга. Но зачем парсить, если они выкладывают актуальный zip документации? )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b68bdfa-94e6-4346-92cd-b02e6d7750dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Найдена ссылка на ZIP-архив: https://scikit-learn.org/stable//_downloads/scikit-learn-docs.zip\n"
     ]
    }
   ],
   "source": [
    "# Загружаем страницу со списком версий\n",
    "versions_url = \"https://scikit-learn.org/dev/versions.html\"\n",
    "response = requests.get(versions_url)\n",
    "response.raise_for_status()\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Находим блок с \"stable\" и ищем ссылку на ZIP\n",
    "zip_link = None\n",
    "for p_tag in soup.find_all('p'):\n",
    "    # Проверяем, есть ли упоминание \"stable\" в тексте\n",
    "    if 'stable' in p_tag.get_text().lower() and 'documentation' in p_tag.get_text().lower():\n",
    "        # Ищем ссылку на ZIP в том же теге <p>\n",
    "        zip_anchor = p_tag.find('a', href=True, string=lambda text: text and 'zip' in text.lower())\n",
    "        if zip_anchor:\n",
    "            zip_link = urljoin(versions_url, zip_anchor['href'])\n",
    "            break\n",
    "\n",
    "if not zip_link:\n",
    "    raise ValueError(\"❌ Не удалось найти ссылку на ZIP-архив stable-документации\")\n",
    "\n",
    "print(f\"✅ Найдена ссылка на ZIP-архив: {zip_link}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09a4a268-9c08-470a-a51a-2b4e8fd7ed3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Скачиваю архив...\n",
      "Архив сохранён как scikit-learn-docs.zip\n",
      "✅ Документация распакована в 'scikit-learn-docs'\n"
     ]
    }
   ],
   "source": [
    "# Скачиваем ZIP\n",
    "zip_path = \"scikit-learn-docs.zip\"\n",
    "print(f\"Скачиваю архив...\")\n",
    "with requests.get(zip_link, stream=True) as r:\n",
    "    r.raise_for_status()\n",
    "    with open(zip_path, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "print(f\"Архив сохранён как {zip_path}\")\n",
    "\n",
    "# Распаковываем\n",
    "import zipfile\n",
    "docs_dir = \"scikit-learn-docs\"\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(docs_dir)\n",
    "print(f\"✅ Документация распакована в '{docs_dir}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626796c4-be24-41b5-9e64-f52e47235ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1b75369-bbb6-4380-9ea4-3790eb45f174",
   "metadata": {},
   "source": [
    "# 2. Предобработка и разделение (Split)\n",
    "https://python.langchain.com/docs/tutorials/rag/\n",
    "\n",
    "1. Фильтрация файлов. Оставляет только полезные разделы, чтобы сократить итоговое кол-во чангов.  \n",
    "\n",
    "2. Очистка контента\n",
    "   - Удаляет навигацию, скрипты, стили через BeautifulSoup.  \n",
    "   - Оставляет только основной текст (заголовки, описания, код).  \n",
    "\n",
    "3. Извлечение метаданных \n",
    "   - Заголовки страниц, «хлебные крошки» (навигация вида User Guide → Model Selection).  \n",
    "   - Приоритезация разделов (например, User Guide важнее API).  \n",
    "\n",
    "4. Разбиение на чанки  \n",
    "   - Использует RecursiveCharacterTextSplitter для семантического разделения текста.  \n",
    "   - Сохраняет контекст (раздел, заголовок) в каждом чанке.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5ca1bda-2915-4b80-9971-1d23bb5915fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScikitDocsPreprocessor:\n",
    "    def __init__(self, docs_dir: str):\n",
    "        self.docs_dir = docs_dir\n",
    "        # Фильтрация ненужных разделов\n",
    "        self.exclude_sections = ['search', 'genindex', '_sources', '_modules', 'changes']\n",
    "        # Приоритет разделов для будущей сортировки/взвешивания\n",
    "        self.section_hierarchy = {\n",
    "            'user_guide': 1,\n",
    "            'modules': 2,\n",
    "            'auto_examples': 3,\n",
    "            'api': 4\n",
    "        }\n",
    "\n",
    "    def extract_breadcrumbs(self, soup: BeautifulSoup) -> str:\n",
    "        \"\"\"Извлекает хлебные крошки для контекста навигации\"\"\"\n",
    "        breadcrumbs = []\n",
    "        for item in soup.select('.bd-breadcrumbs .nav-link'):\n",
    "            text = item.get_text(strip=True)\n",
    "            if text:\n",
    "                breadcrumbs.append(text)\n",
    "        return \" → \".join(breadcrumbs) if breadcrumbs else \"User Guide\"\n",
    "\n",
    "    def extract_page_title(self, soup: BeautifulSoup) -> str:\n",
    "        \"\"\"Извлекает заголовок страницы с приоритетом: og:title → .title → <title>\"\"\"\n",
    "        og_title = soup.find('meta', property='og:title')\n",
    "        if og_title:\n",
    "            return og_title.get('content', '').strip()\n",
    "\n",
    "        p_title = soup.find('p', class_='title')\n",
    "        if p_title:\n",
    "            return p_title.get_text(strip=True)\n",
    "\n",
    "        if soup.title and soup.title.string:\n",
    "            return soup.title.string.strip()\n",
    "\n",
    "        return os.path.basename(self.docs_dir)\n",
    "\n",
    "    def clean_html_content(self, soup: BeautifulSoup) -> str:\n",
    "        \"\"\"Очищает HTML от навигации, скриптов и оставляет только основной контент\"\"\"\n",
    "        selectors_to_remove = [\n",
    "            'nav', 'footer', 'script', 'style', 'aside', 'header', 'iframe', 'svg',\n",
    "            'button', '.sidebar', '.toc', '.admonition', '.bd-header-article',\n",
    "            '.bd-footer', '.bd-toc', '.pst-scrollable-table-container'\n",
    "        ]\n",
    "        for selector in selectors_to_remove:\n",
    "            for el in soup.select(selector):\n",
    "                el.decompose()\n",
    "\n",
    "        # Поиск основного контента\n",
    "        main = (\n",
    "            soup.find('main') or\n",
    "            soup.find('article') or\n",
    "            soup.find('div', class_='bd-article-container') or\n",
    "            soup.find('div', class_='document') or\n",
    "            soup.body\n",
    "        )\n",
    "\n",
    "        if main:\n",
    "            # Удаляем атрибуты для уменьшения шума\n",
    "            for tag in main.find_all(True):\n",
    "                tag.attrs = {}\n",
    "            text = main.get_text()\n",
    "        else:\n",
    "            text = soup.get_text()\n",
    "\n",
    "        # Нормализация пробелов\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "\n",
    "    def is_relevant_document(self, file_path: str) -> bool:\n",
    "        \"\"\"Фильтрует нерелевантные HTML-файлы\"\"\"\n",
    "        return (\n",
    "            file_path.endswith('.html') and\n",
    "            not any(excl in file_path for excl in self.exclude_sections) and\n",
    "            os.path.getsize(file_path) > 512  # Минимальный размер\n",
    "        )\n",
    "\n",
    "    def load_and_preprocess(self) -> List[Document]:\n",
    "        \"\"\"Загружает и предобрабатывает все HTML-файлы\"\"\"\n",
    "        documents = []\n",
    "\n",
    "        # Сбор всех HTML-файлов\n",
    "        html_files = []\n",
    "        for root, _, files in os.walk(self.docs_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                if self.is_relevant_document(file_path):\n",
    "                    html_files.append(file_path)\n",
    "\n",
    "        print(f\"Найдено {len(html_files)} релевантных HTML-файлов\")\n",
    "\n",
    "        for file_path in tqdm(html_files, desc=\"Обработка HTML\"):\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    soup = BeautifulSoup(f, 'html.parser')\n",
    "\n",
    "                # Извлечение контекста\n",
    "                breadcrumbs = self.extract_breadcrumbs(soup)\n",
    "                page_title = self.extract_page_title(soup)\n",
    "                relative_path = os.path.relpath(file_path, self.docs_dir)\n",
    "                section = relative_path.split(os.sep)[0] if os.sep in relative_path else 'root'\n",
    "\n",
    "                # Очистка контента\n",
    "                content = self.clean_html_content(soup)\n",
    "\n",
    "                # Добавление семантического контекста\n",
    "                context = (\n",
    "                    f\"[Breadcrumb: {breadcrumbs}]\\n\"\n",
    "                    f\"[Page: {page_title}]\\n\"\n",
    "                    f\"[Section: {section}]\\n\\n\"\n",
    "                )\n",
    "                full_content = context + content\n",
    "\n",
    "                metadata = {\n",
    "                    'source': file_path,\n",
    "                    'section': section,\n",
    "                    'section_priority': self.section_hierarchy.get(section, 99),\n",
    "                    'breadcrumbs': breadcrumbs,\n",
    "                    'page_title': page_title,\n",
    "                    'relative_path': relative_path\n",
    "                }\n",
    "\n",
    "                documents.append(Document(page_content=full_content, metadata=metadata))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nОшибка при обработке {file_path}: {str(e)}\")\n",
    "\n",
    "        return documents\n",
    "\n",
    "    def chunk_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"Разбивает документы на чанки с сохранением семантики\"\"\"\n",
    "        # Важно: используем правильные разделители для документации\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            # Порядок важен: сначала по абзацам, потом по предложениям, потом по строкам\n",
    "            separators=[\n",
    "                \"\\n\\n\",           # Абзацы\n",
    "                \"\\n\",             # Параграфы\n",
    "                \". \",             # Конец предложения\n",
    "                \"! \", \"? \",\n",
    "                \" \",              # Слова\n",
    "                \"\"                # По символам (в крайнем случае)\n",
    "            ],\n",
    "            keep_separator=True,\n",
    "            strip_whitespace=True\n",
    "        )\n",
    "\n",
    "        print(\"Разбиение на чанки...\")\n",
    "        chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "        # Постобработка: очистка от артефактов\n",
    "        for chunk in chunks:\n",
    "            content = chunk.page_content\n",
    "\n",
    "            # Удаление примечаний, ссылок и лишних пробелов\n",
    "            content = re.sub(r'Note\\s+.*?(?=\\n\\n|\\Z)', '', content, flags=re.DOTALL)\n",
    "            content = re.sub(r'\\[\\d+\\]', '', content)  # Ссылки вроде [1]\n",
    "            content = re.sub(r'\\s+', ' ', content)     # Лишние пробелы\n",
    "            content = re.sub(r'\\n+', '\\n', content).strip()\n",
    "\n",
    "            chunk.page_content = content\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def process(self) -> List[Document]:\n",
    "        \"\"\"Полный пайплайн: загрузка → очистка → разбиение\"\"\"\n",
    "        print(\"Запуск предобработки документации scikit-learn...\")\n",
    "        docs = self.load_and_preprocess()\n",
    "        chunks = self.chunk_documents(docs)\n",
    "        print(f\"✅ Обработка завершена. Получено {len(chunks)} чанков.\")\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f3156f6-a312-49e5-8430-c5e742eabab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запуск предобработки документации scikit-learn...\n",
      "Найдено 1080 релевантных HTML-файлов\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка HTML: 100%|█████████████████████████| 1080/1080 [02:26<00:00,  7.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Разбиение на чанки...\n",
      "✅ Обработка завершена. Получено 10602 чанков.\n"
     ]
    }
   ],
   "source": [
    "# Запуск\n",
    "processor = ScikitDocsPreprocessor(docs_dir=\"scikit-learn-docs\")\n",
    "processed_chunks = processor.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f3465f0-9cfd-41c6-9384-db47518fd53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Пример 1 ---\n",
      "Источник: scikit-learn-docs/related_projects.html\n",
      "Контекст: [Breadcrumb: User Guide] [Page: Related Projects] [Section: root]...\n",
      "\n",
      "--- Пример 2 ---\n",
      "Источник: scikit-learn-docs/related_projects.html\n",
      "Контекст: Related Projects# Projects implementing the scikit-learn estimator API are encouraged to use the scikit-learn-contrib template which facilitates best practices for testing and documenting estimators. The scikit-learn-contrib GitHub organization also accepts high-quality contributions of repositories...\n"
     ]
    }
   ],
   "source": [
    "# Пример\n",
    "for i, chunk in enumerate(processed_chunks[:2]):\n",
    "    print(f\"\\n--- Пример {i+1} ---\")\n",
    "    print(f\"Источник: {chunk.metadata['source']}\")\n",
    "    print(f\"Контекст: {chunk.page_content[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a15fb3-3cfe-453f-bfa7-405b37b0aa3c",
   "metadata": {},
   "source": [
    "На данном этапе источник — это имя скаченного файла документации, в итоговом представлении преобразуем его в URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57120644-a417-4309-a35e-dc4e53e4abf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Чанки сохранены\n"
     ]
    }
   ],
   "source": [
    "# Сохранение\n",
    "import dill\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "with open(\"output/scikit_chunks.pkl\", \"wb\") as f:\n",
    "    dill.dump(processed_chunks, f)\n",
    "print(\"✅ Чанки сохранены\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59558c7-4228-4da4-accd-9fd52e82d875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67a7a9ec-291c-4ed1-bfe9-2075364d9fc5",
   "metadata": {},
   "source": [
    "# 3. Векторизация (Embed) и FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650c5341-8595-4474-9ff1-823c5d5b9262",
   "metadata": {},
   "source": [
    "Чтобы сгенерировать эмбеддинги необходимо обратиться к локальному серверу Ollama, который должен быть запущен отдельно. Выполнить в терминале:\n",
    "\n",
    "    ollama pull bge-m3\n",
    "    ollama run bge-m3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "583d71ee-ce8e-4c1c-bce9-f7f27dc762a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODOO Так же можно протестировать и другие модели, например all-minilm, nomic-embed-text, mxbai-embed-large, но bge-m3 — одна из лучших для RAG. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "61f172a7-1423-45c0-be28-14dbd8f96e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Создание FAISS: 100%|███████████████████████████| 213/213 [58:47<00:00, 16.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Векторное хранилище построено\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Запустить в терминале: ollama run bge-m3\n",
    "\n",
    "# Эмбеддинги\n",
    "embeddings = OllamaEmbeddings(model=\"bge-m3\")\n",
    "\n",
    "# Построение FAISS\n",
    "db = None\n",
    "batch_size = 50\n",
    "for i in tqdm(range(0, len(processed_chunks), batch_size), desc=\"Создание FAISS\"):\n",
    "    batch = processed_chunks[i:i + batch_size]\n",
    "    if db is None:\n",
    "        db = FAISS.from_documents(batch, embeddings)\n",
    "    else:\n",
    "        db.add_documents(batch, embeddings=embeddings)\n",
    "\n",
    "print(\"✅ Векторное хранилище построено\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d49b3538-2946-4b16-8735-020ae3620c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Сохранено в output/scikit_faiss_index\n"
     ]
    }
   ],
   "source": [
    "# Сохранение векторов\n",
    "def safe_save_faiss(db, folder_path):\n",
    "    \"\"\"Сохранение FAISS с обработкой проблемных метаданных\"\"\"\n",
    "    for doc_id in db.index_to_docstore_id.values():\n",
    "        doc = db.docstore._dict[doc_id]\n",
    "        try:\n",
    "            pickle.dumps(doc)\n",
    "        except Exception:\n",
    "            # Конвертируем все значения метаданных в строки (чтобы избежать ошибок сериализации)\n",
    "            doc.metadata = {k: str(v) for k, v in doc.metadata.items()}\n",
    "    \n",
    "    # Увеличиваем лимит рекурсии для больших индексов иначе падает\n",
    "    sys.setrecursionlimit(10000)\n",
    "    \n",
    "    # Сохраняем векторное хранилище\n",
    "    db.save_local(folder_path)\n",
    "    print(f\"✅ Сохранено в {folder_path}\")\n",
    "\n",
    "# Вызов функции\n",
    "safe_save_faiss(db, \"output/scikit_faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a76d5eb-133a-48cd-b99c-6afd2c40d472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33b3af2d-1f91-4d8e-9b74-921258677684",
   "metadata": {},
   "source": [
    "# 4. Настройка Retriever и LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2c0f1a7d-9d2d-4676-b6e0-3bee21882707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настройка Retriever\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 4})  # 4 релевантных чанка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "baa99171-3d76-4499-be65-beff3d2cc0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подключение LLM (через Ollama) \n",
    "# В терминале: ollama run llama3 - держать запущенным во время работы RAG-интерфейса.\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3\", temperature=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c572b1-1487-47d6-a5ff-8688366813f0",
   "metadata": {},
   "source": [
    "# 5. Сборка RAG-цепочки (Chain)\n",
    "\n",
    "Для улучшения тестируемых промтов использован https://neuralwriter.com/ru/prompt-tool/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "36d9493a-ad8a-4285-8daf-9c17240d47c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Вы — эксперт по машинному обучению и библиотеке scikit-learn.\n",
    "Отвечайте на вопрос на русском языке, опираясь только на следующий контекст.\n",
    "Если в контексте нет ответа, скажите: \"На основе документации scikit-learn ответить не могу.\"\n",
    "\n",
    "Контекст:\n",
    "{context}\n",
    "\n",
    "Вопрос:\n",
    "{question}\n",
    "\n",
    "Ответ:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd47ca2-9882-4c79-801a-3433db7c0263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c6743210-f21e-413e-b631-013016843144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для преобразования локального пути в URL\n",
    "def local_path_to_url(local_path: str, base_url: str = \"https://scikit-learn.org/stable\") -> str:\n",
    "    relative_path = \"/\".join(local_path.split(\"/\")[1:])  # убираем 'scikit-learn-docs/'\n",
    "    return f\"{base_url.rstrip('/')}/{relative_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d6bb2fc9-bfc6-4dd4-a1fc-2ad61170c068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://scikit-learn.org/stable/modules/clustering.html\n"
     ]
    }
   ],
   "source": [
    "# Тестируем:\n",
    "\n",
    "# Пример\n",
    "local_path = \"scikit-learn-docs/modules/clustering.html\"\n",
    "url = local_path_to_url(local_path)\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c79456f-b092-48fc-8506-93d59625d3e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91ab8eab-dec0-4cb7-9ccc-a7f14c3fa631",
   "metadata": {},
   "source": [
    "# 6. Демонстрация: 3 примера с анализом"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859c53b0-a86c-4e02-9443-6b8a82167ff4",
   "metadata": {},
   "source": [
    " ## Пример 1: Поиск факта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d3cf0dea-e717-4438-ae61-1f5cca3bfccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ: Параметр `n_clusters` в классе `KMeans` отвечает за количество кластеров. Он имеет тип `int` и по умолчанию равен 8.\n",
      "\n",
      "Источники:\n",
      "- https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
      "- https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html\n"
     ]
    }
   ],
   "source": [
    "question = \"Какой параметр в KMeans отвечает за количество кластеров?\"\n",
    "response = rag_chain.invoke(question)\n",
    "\n",
    "docs = db.similarity_search(question, k=2)\n",
    "sources = \"\\n\".join(f\"- {local_path_to_url(d.metadata['source'])}\" for d in docs)\n",
    "\n",
    "print(f\"Ответ: {response}\\n\\nИсточники:\\n{sources}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e298bf-0bd0-4911-9e58-ac8dde7d9c1e",
   "metadata": {},
   "source": [
    "✅ Анализ:\n",
    "\n",
    "Запрос прямой, факт содержится в докумментации. RAG система точно извлекла информацию из правильного источника. Это демонстрирует хорошую точность ретривера и качество эмбеддингов.\n",
    "\n",
    "Первый источник — sklearn.cluster.KMeans.html — идеален: это прямая API-документация, где описан параметр n_clusters.\n",
    "Второй источник — пример кластеризации текстов — релевантен, но косвенный: используется KMeans, но не объясняется параметр. Тем не менее, это хороший контекст."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df51bb68-5d8a-443d-b658-6f6cf0e25c67",
   "metadata": {},
   "source": [
    " ## Пример 2: Синтез информации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "cfb30de6-5e61-4bf4-8aec-498d7580b1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ: КMeans и MiniBatchKMeans - это оба алгоритмы для кластеризации, но они отличаются по своей природе.\n",
      "\n",
      "KMeans - это классический алгоритм кластеризации, который учитывает все данные в выборке. Он работает с помощью итеративного процесса, где центроиды кластеров обновляются на основе средних значений для каждого кластера. Каждый шаг алгоритма занимает время O(n), где n - количество данных.\n",
      "\n",
      "MiniBatchKMeans - это модифицированная версия KMeans, которая работает с mini-батчами (мини-выборками) из данных, а не со всеми данными сразу. Это позволяет ускорить процесс кластеризации, особенно для больших выборок. MiniBatchKMeans также использует итеративный процесс, но каждый шаг занимает время O(m), где m - размер mini-батча.\n",
      "\n",
      "В целом, KMeans обеспечивает более точные результаты, но может быть медленнее, чем MiniBatchKMeans, для больших выборок. MiniBatchKMeans, с другой стороны, может быть быстрее, но может привести к менее точным результатам из-за ограничения mini-батча.\n",
      "\n",
      "Источники:\n",
      "- https://scikit-learn.org/stable/auto_examples/cluster/plot_mini_batch_kmeans.html\n",
      "- https://scikit-learn.org/stable/modules/clustering.html\n"
     ]
    }
   ],
   "source": [
    "question = \"Чем отличаются KMeans и MiniBatchKMeans?\"\n",
    "response = rag_chain.invoke(question)\n",
    "\n",
    "docs = db.similarity_search(question, k=2)\n",
    "sources = \"\\n\".join(f\"- {local_path_to_url(d.metadata['source'])}\" for d in docs)\n",
    "\n",
    "print(f\"Ответ: {response}\\n\\nИсточники:\\n{sources}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a197fd3a-470c-4e5f-95c5-fad8f10accd3",
   "metadata": {},
   "source": [
    "✅  Анализ:\n",
    "\n",
    "\n",
    "Ответ корректно отражает ключевые различия:   Точно описаны основные шаги KMeans, правильно отмечен компромисс между скоростью и точностью применении MiniBatchKMeans.  В целом хорошее сравнение. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3347d0-0d2d-43f8-81ce-1bb48bcadc6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da90cd1f-bc81-4500-bfe3-71d60d3ec2c8",
   "metadata": {},
   "source": [
    " ## Пример 3: Сложный/негативный случай\n",
    "\n",
    " Спрашиваем информацию, которой нет в документации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "950b533e-505a-4951-b75c-6a7a0738aa14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ: На основе документации scikit-learn ответить не могу.\n",
      "\n",
      "Источники:\n",
      "- https://scikit-learn.org/stable/faq.html\n",
      "- https://scikit-learn.org/stable/related_projects.html\n"
     ]
    }
   ],
   "source": [
    "question = \"Как scikit-learn поддерживает обучение с подкреплением?\"\n",
    "response = rag_chain.invoke(question)\n",
    "\n",
    "docs = db.similarity_search(question, k=2)\n",
    "sources = \"\\n\".join(f\"- {local_path_to_url(d.metadata['source'])}\" for d in docs)\n",
    "\n",
    "print(f\"Ответ: {response}\\n\\nИсточники:\\n{sources}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2b1962-c95b-4335-a081-6dcef4b1e21a",
   "metadata": {},
   "source": [
    "✅ Анализ:\n",
    "\n",
    "LLM не галлюцинирует, а честно говорит, что не знает — хорошее поведение.\n",
    "Ответ верно отражает, что scikit-learn изначально не поддерживает обучение с подкреплением, приведена корректная цитата из FAQ. \n",
    "Указаны релевантные разделы документации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddefd980-960b-4cf4-829b-3646a0e063a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c13ede5-59c0-43cf-a1cd-7d191fb80cab",
   "metadata": {},
   "source": [
    "# Пример 3.2\n",
    "Еще один сложный случай: домен программирования, объемный запрос, который не имеет единого ответа в документации — это композиция знаний из разных разделов.\n",
    "\n",
    "Запрос: Создай полный пайплайн для работы с моделью классификации, включающий загрузку и предобработку данных с учетом дисбаланса классов, кросс-валидацию на 5 фолдах и получение предсказаний , используя все возможности scikit-learn. Прокомментируте свой выбор методов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "6610f20f-4512-4f7d-8f07-6cd3e5b0164c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ: Вот полный пайплайн для работы с моделью классификации, включающий загрузку и предобработку данных с учетом дисбаланса классов, кросс-валидацию на 5 фолдах и получение предсказаний:\n",
      "\n",
      "```\n",
      "from sklearn.datasets import load_iris\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.preprocessing import StandardScaler, ImbalancedDatasetSampler\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "# Загрузка данных\n",
      "iris = load_iris()\n",
      "X, y = iris.data, iris.target\n",
      "\n",
      "# Разделение на обучающую и тестовую выборки\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
      "\n",
      "# Создание пайплайна с использованием StandardScaler и LogisticRegression\n",
      "pipe = Pipeline([\n",
      "    ('standardscaler', StandardScaler()),\n",
      "    ('logisticregression', LogisticRegression())\n",
      "])\n",
      "\n",
      "# Фитинг пайплайна на обучающей выборке\n",
      "pipe.fit(X_train, y_train)\n",
      "\n",
      "# Кросс-валидация на 5 фолдах\n",
      "scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
      "print(\"Кросс-валидационные оценки:\", scores)\n",
      "print(\"Средняя кросс-валидационная оценка:\", scores.mean())\n",
      "\n",
      "# Получение предсказаний на тестовой выборке\n",
      "y_pred = pipe.predict(X_test)\n",
      "\n",
      "# Оценка точности модели\n",
      "accuracy = accuracy_score(y_test, y_pred)\n",
      "print(\"Точность модели:\", accuracy)\n",
      "```\n",
      "\n",
      "В моем выборе методов я использовал:\n",
      "\n",
      "* `StandardScaler` для нормализации признаков;\n",
      "* `ImbalancedDatasetSampler` для учета дисбаланса классов;\n",
      "* `LogisticRegression` как классификатор;\n",
      "* `Pipeline` для объединения этапов предобработки и обучения модели;\n",
      "* `cross_val_score` для кросс-валидации на 5 фолдах;\n",
      "* `accuracy_score` для оценки точности модели.\n",
      "\n",
      "Я выбрал эти методы потому, что они позволяют эффективно решать задачу классификации с учетом дисбаланса классов и обеспечивают высокую точность предсказаний.\n",
      "\n",
      "Источники:\n",
      "- https://scikit-learn.org/stable/related_projects.html\n",
      "- https://scikit-learn.org/stable/getting_started.html\n",
      "- https://scikit-learn.org/stable/data_transforms.html\n",
      "- https://scikit-learn.org/stable/model_persistence.html\n"
     ]
    }
   ],
   "source": [
    "question = \"Создай полный пайплайн для работы с моделью классификации, включающий загрузку и предобработку данных с учетом дисбаланса классов, кросс-валидацию на 5 фолдах и получение предсказаний , используя все возможности scikit-learn. Прокомментируте свой выбор методов.\"\n",
    "response = rag_chain.invoke(question)\n",
    "\n",
    "docs = db.similarity_search(question, k=4)\n",
    "sources = \"\\n\".join(f\"- {local_path_to_url(d.metadata['source'])}\" for d in docs)\n",
    "\n",
    "print(f\"Ответ: {response}\\n\\nИсточники:\\n{sources}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d60f2a7-33db-4f90-bf05-f2fb41af87a8",
   "metadata": {},
   "source": [
    "✅ Анализ:\n",
    "\n",
    "\n",
    "Вот детальный анализ ответа RAG-системы на запрос о пайплайне классификации:\n",
    "\n",
    "Основные проблемы ответа: \n",
    "\n",
    "1. **Фактические ошибки**:\n",
    "   - **ImbalancedDatasetSampler** не существует в scikit-learn (ошибка генерации)\n",
    " Набор данных Iris не имеет дисбаланса классов, но модель этого действительно не знает, взяла датасет, на котором в документации показана работа с классами. \n",
    "\n",
    "2. **Неполное решение**:\n",
    "\n",
    " - Нет стратификации при разделении на train и test\n",
    "  - Нет реальных методов обработки дисбаланса (SMOTE, классовые веса и т.д.)\n",
    "  - Отсутствует подбор гиперпараметров (хотя при других генерациях предлагалось использовать GridSearchCV)\n",
    "  - Нет других метрик, кроме accuracy (максимально неэффективная метрика при дисбалансе :) )\n",
    "    \n",
    "4. **Проблемы с источниками**:\n",
    " Ссылки слишком общие (разделы getting_started и data_transforms), почему-то при данной генерации не рассматривался, например более конкретный  раздел https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html   \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3678e1a-eae8-4345-88ab-5599b02fffe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1f89922-1bef-434f-81af-c8bfd6c976de",
   "metadata": {},
   "source": [
    "# Gradio-интерфейс для презентации RAG-системы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7a3c37cd-8375-494a-a2c4-5b60b72d4b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO В терминале: ollama run llama3 - держать запущенным во время работы RAG-интерфейса."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37640324-86b5-40ce-9b94-031b9f74469f",
   "metadata": {},
   "source": [
    "Добавим функцию преобразования локального пути в URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "605d4b8a-23e0-48ab-b20d-694cf9bbcd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(question, progress=gr.Progress()):\n",
    "    if not question.strip():\n",
    "        return \"Введите вопрос по scikit-learn.\"\n",
    "        \n",
    "    try:\n",
    "        progress((1, 2), \"Поиск в базе знаний...\")\n",
    "        time.sleep(0.1)  # минимальная задержка, чтобы прогресс отобразился\n",
    "\n",
    "        # Поиск релевантных чанков\n",
    "        docs = db.similarity_search(question, k=2)\n",
    "\n",
    "        progress((2, 2), \"Генерация ответа...\")\n",
    "        time.sleep(0.1)\n",
    "\n",
    "        # Генерация ответа\n",
    "        response = rag_chain.invoke(question)\n",
    "\n",
    "        # Форматируем источники как кликабельные ссылки\n",
    "        sources = \"\\n\".join([\n",
    "            f\"🔗 [{os.path.basename(d.metadata['source'])}]({local_path_to_url(d.metadata['source'])})\"\n",
    "            for d in docs\n",
    "        ])\n",
    "        \n",
    "        return f\"{response}\\n\\n---\\n**Источники:**\\n{sources}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"❌ Ошибка: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "8400c567-3ef8-4527-b31e-594276ddd77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = gr.Interface(\n",
    "    fn=answer,\n",
    "    inputs=gr.Textbox(\n",
    "        placeholder=\"Например: Какой параметр в KMeans отвечает за количество кластеров?\",\n",
    "        label=\"Ваш вопрос\",\n",
    "        lines=2\n",
    "    ),\n",
    "    outputs=gr.Markdown(label=\"Ответ\"),\n",
    "    title=\"🔍 RAG по документации scikit-learn\",\n",
    "    description=\"\"\"\n",
    "    Задайте вопрос о кластеризации, preprocessing, моделях и других аспектах scikit-learn.\n",
    "    Система найдёт ответ в официальной документации.\n",
    "    \"\"\",\n",
    "    examples=[\n",
    "        [\"Какой параметр в KMeans отвечает за количество кластеров?\"],\n",
    "        [\"Чем отличаются DBSCAN и KMeans?\"],\n",
    "        [\"Почему scikit-learn не поддерживает графовые нейронные сети?\"]\n",
    "    ],\n",
    "    theme=gr.themes.Soft(),\n",
    "    flagging_mode=\"never\",\n",
    "    # allow_flagging=\"never\",\n",
    "    clear_btn=\"Очистить\",\n",
    "    submit_btn=\"Отправить\"\n",
    ")\n",
    "\n",
    "demo.launch(debug=True, share=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e519001b-c2de-4c1f-9f6a-d45b342afa7f",
   "metadata": {},
   "source": [
    "Создайте полный пайплайн для работы с моделью классификации, включающий загрузку и предобработку данных с учетом дисбаланса классов, кросс-валидацию на 5 фолдах и получение предсказаний , используя все возможности scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92267157-1057-458d-9633-30b9f6ce67a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29713fcd-d84c-477f-a24b-ef37ce8801c9",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d59e56-cd72-45be-b141-d14b84eabf05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
